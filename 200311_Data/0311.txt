**일반적인 python 이나 anaconda를 이용해서 설치한 파이썬 - c로 만들어진 python
=>c 언어로만든 라이브러리를 파이썬에서 사용할 수 있음
=>c 언어는 소스코드를 가지고 실행 파일을 만들어서 배포
실행 파일을 만들 때 운영체제의 start up 코드가 포함되어야 합니다.
실행되는 프로그램이 운영체제마다 다릅니다.
=>windows에서 실행되는 C언어는 MS-C 이고 이 언어로 프로그램을 만드는 대표적인 IDE 가 Visual C++ 이고 이 Visual C++로 만들어진 프로그램을 실행시키기 위해서 재배포 패키지나 build tool이 설치되어 있어야 합니다.

**windows에서 python 실행 시 라이브러리가 설치되지 않는데 
visual c++ 14.0 이 설치되어 있어야 하는데 설치되어 있지 않다는 에러 메시지가 출력되는 경우
 - visual studio 2015 재배포 패키지 설치 : 설치해도 잘 안됨
 - visual studio 2015 build tools 설치 : 설치해도 잘 안됨
 - visual studio 최신 버전 설치 : visual studio community 버전 다운로드

**불균형한(개수가 다른 경우) 클래스의 데이터를 가지고 분석을 해야 하는 경우
=>샘플 자체의 개수가 작을 때는 자료를 더 수집하는 것이 가장 좋은 방법
=>데이터를 수집하는 것이 가능하지 않을 때는 데이터에 가중치를 적용해서 사용 - 분류 알고리즘의 매개변수 중에서 weight가 있으면 이 매개변수가 데이터에 가중치를 적용할 수 있는 매개변수입니다.
=>개수가 작은 데이터의 개수를 강제로 늘리거나 개수가 많은 데이터의 개수를 줄이는 업샘플링이나 다운 샘플링을 해서 알고리즘에 적용
=>평가지표를 다양하게 선택 - 정확도 대신에 재현율이나 F1 통계량 등을 이용

**다변량 분석에서 데이터의 상대적 크기 문제
=>다변량 분석 - 2개이상의 컬럼의 데이터를 가지고 분석
=>하나의 컬럼의 데이터는 값의 범위가 0-100 이고 다른 컬럼의 데이터는 값의 범위가 0-1 이라면 이 경우 2개의 데이터를 가지고 다변량 분석을 하게되면 첫번째 컬럼의 영향을 받게 될 수 있습니다.
이런 경우에는 값의 범위를 일치시켜 주는것이 좋습니다.
=>값의 범위는 같은데 분포가 다른 경우에도 분포를 기준으로 해서 값을 조정할 필요가 있습니다.
최대값으로 나누거나 최대값-최소값을 분모로하고 해당값-최소값을 분자로 해서 값을 조정
=>이러한 값의 조정을 scailing 이라고 합니다.
0~1 사이나 -1~1사이로 조정합니다.
더 큰 값으로 가능하지만 머신러닝 모델에서는 값의 크기가 커지면 정확도가 떨어집니다.

1.표준화
=>모든 값들의 표준 값을 정해서 그 값을 기준으로 차이를 구해서 비교하는 방법
1)표준값: (데이터-평균)/표준편차 - 표준값의 평균은 50
2)편차값: 표준값 * 10 + 50 - 위의 숫자보다 큰 숫자로 변환

2.sklearn 의 정규화 
1)StandardScaler: 평균이 0이고 표준편차가 1이 되도록 변환
(벡터-평균)/표준편차
주로 주성분분석에서 많이 이용

2)MinMaxScaler: 최대값이 1 최소값이 0이 되도록 변환
(벡터-최소값)/(최대값-최소값)
신경망에서 주로 이용

3)RobustScaler: 중앙값이 0 IQR(4분위수)를 이용하는 방식
(벡터-중간값)/(75%-25%)
=>앞의 방식들은 outlier(극단치)에 영향을 많이 받습니다.
=>데이터의 분포가 불균형이거나 극단치가 존재하는 경우에 주로 이용

4)QuantileTransformer: 데이터를 1000개의 분위로 나눈 후 0-1 사이에 고르게 분포시키는 방식
=>outlier의 영향을 적게 받기 위해서 사용

**정규화
=>값의 범위를 0-1 사이의 데이터로 변환
=>표준화는 일정한 범위 내로 데이터를 변환하는 것이고 정규화는 0-1 사이로 해야 합니다.
=>Nomalizer 클래스를 이용해서 transform 메소드에 데이터를 대입하면 됩니다.
이 때 norm 매개변수에 옵션을 설정할 수 있는데 l1, l2, max 등의 값을 설정할 수 있습니다.
max는 최대값으로 나누는 방식
max를 이용할 때는 하나의 부호 형태의 데이터이어야 합니다.
l1과 l2는 거리 계산 방식
l1- 맨하턴 거리를 이용하고 l2는 유클리드 거리를 이용

**다항과 교차항 특성
=>기존 데이터에 데이터들을 곱하고 제곱을 해서 데이터를 추가하는 것
=>특성과 타켓 사이에 비선형 관계과 존재할 때 사용하는 방식
=>비선형 관계는 2개의 관계가 직선의 형태가 아니고 곡선의 형태인 것
=>각 특성이 다른 특성에 영향을 줄 때 각 특성을 곱합 교차항을 가지고 인코딩
=>다변량 분석(2개 이상의 컬럼을 가지고 분석)을 할 때 2개의 컬럼 사이에 상관관계가 있는 경우가 있는데 이런 경우 2개의 컬럼 모두를 가지고 분석을 하게 되면 다중공선성 문제가 발생할 수 있습니다.
어떤 컬럼의 값을 알면 다른 컬럼의 값을 예측할 수 있는 경우 발생할 수 있는 문제입니다.
이런 경우에는 2개의 컬럼을 1개의 컬럼으로 변환하는 작업을 해야 하는데 (차원축소) 이런 경우 더하거나 곱하거나 제곱해서 새로운 값을 만들어 냅니다. 

=>PolynomialFeatures 클래스를 이용하는데 몇 차 항까지 생성할 것인지 degree에 설정
첫번째 데이터로 1을 추가할 지 여부를 include_bias에 설정
=>연산식의 순서는 get_feature_names 메소드를 이용해서 확인 가능

**표준화나 정규화는 직접 하는 경우가 많지만 다항식을 만드는 것은 머신러닝 알고리즘에서 자체적으로 처리하는 경우가 많음

**특성 변환
=>데이터에 동일한 함수를 적용해서 다른 데이터로 직접 변환하는 것
=>pandas에서는 apply 메소드를 이용하고 sklearn에서는 preprocessing.FunctionTransformer 나 ColumnTransformer 클래스를 이용
=>FunctionTransformer 는 모든 열에 동일한 함수를 적용하고 ColumnTransformer는 서로 다른 함수를 적용할 수 있습니다.
객체를 생성할 때 적용할 함수를 설정해서 만들고 transform 메소드에 데이터를 대입하면 됩니다.

**Outlier 감지
=>Outlier: 이상치나 극단치, 일반적인 데이터의 범위를 넘어선 값
1.z 점수를 이용하는 방법: 중앙값을 기준으로 표준편차가 3 또는 -3 범위의 바깥쪽에 있는 데이터를 Outlier로 간주
2.z 점수의 보정: z 점수는 데이터가 12개 이하이면 감지를 못함
편차의 범위를 3.5로 늘리고 0.6745를 곱한 값을 이용
3.IQR(3사분위수 - 1사분위수)이용: 1사분위수(25%)보다 1.5 IQR 작은 값이나 3사분위수(75%) 보다 1.5IQR 큰 데이터를 Outlier로 간주
 
**Outlier 처리
1.제거
=>설문조사를 했는데 이상한 데이터가 입력된 것 같은 경우
=>분석 목적에 맞지 않는 데이터인 경우

2.이상한 데이터로 표현해두고 특성의 하나로 간주
=>따로 분석을 수행

3.outlier의 영향이 줄어들도록 특성을 변환 - 값의 범위를 줄임(표준화, 정규화 등)
=>표준화할 때는 RobustScaler를 이용하는 것이 좋음

**시계열 데이터
=>날짜 및 시간에 관련된 데이터
1.pandas의 시계열 자료형
=>datatime64: 부등호를 이용해서 크기비교를 할 수 있고 -를 이용해서 뺄셈을 할 수 있음
=>Period: 두 개의 날짜 사이의 간격을 나타내기 위한 자료형
=>시계열 자료형을 별도로 구성하는 이유는 일정한 패턴을 만들기 쉽도록 하기 위해서

2.datatime64 생성
=>문자열 데이터를 시계열로 변경: pandas.to_datetime() 이용
날짜 형식의 문자열과 format 매개변수에 날짜 형식을 대입

3.Period
=>pandas.to_period 함수를 이용해서 datatime을 Period로 생성
freq 옵션에 기준이 되는 기간을 설정
=>freq 옵션
D: 1일
W: 1주
M: 1개월(월말 기준)
MS: 1개월(월초 기준)
Q: 분기말
QS:분기초
A:연말
AS: 초
B: 휴일 제외
H, T, S, L, U, N: 시간, 분, 초, 밀리초, 마이크로초, 나노초

5.date_range()
=>일정한 간격을 소유한 날짜 데이터의 집합을 생성
=>매개변수
 start: 시작 날짜
 end:종료 날짜
 periods: 생성할 날짜 개수
 freq: 간격
 tz: 시간대

6.날짜 데이터이서 필요한 부분 추출하기
=>dt.year, dt.month, dt.day...
=>요일은 dt.weekday_name 은 문자열로 dt.weekday 하면 숫자로 리턴(월요일이 0)
화면에 출력할 때는 문자열로 하고 머신러닝에 사용할 때는 숫자로 리턴 받습니다.

7.Python에서는 날짜는 datetime 패키지의 datetime 으로 제공
=>날짜 형식의 문자열을 가지고 날짜 형식의 데이터를 생성

8.shift 함수를 이용하면 기존 날짜를 이동시키는 것이 가능
=>freq 매개변수에 간격을 설정할 수 있습니다.

9.resampling
=>시계열의 빈도를 변환하는 것
=>상위 빈도의 데이터를 하위 빈도의 데이터로 변환하는 것을 다운샘플링이라고 하고 반대의 과정을 업 샘플링이라고 합니다.
=>resampling(freq, how, fill_method, closed, label, kind)
    freq: 리샘플링 빈도(M, Q, A 등)
    how: 집계함수를 지정하는 것으로 기본은 mean이고 first, last, max, median, min 등
    fill_method: 업 샘플링할 때 데이터를 채우는 옵션으로 기본은 None 인데 ffill 이나 bfill을 설정해서 이전값이나 이후값으로 채울 수 있음
    closed: 다운 샘플링을 할 때 왼쪽과 오른쪽 어느쪽을 포함시킬지 설정
    label: 다운 샘플링을 할 때 레이블을 왼쪽 또는 오른쪽을 사용할 것인지 여부

10.날짜 데이터가 데이터프레임에 존재하는 경우 날짜 데이터를 인덱스로 설정하면 특정 시간단위로 집계를 하는 것이 쉬워집니다.

**이동시간 윈도우
=>통계적 수치를 계산 할 때 최근의 데이터에 가중치를 부여해야 한다라는 개념
평균을 구할 때 데이터 전체의 평균을 구하는 것 보다는 최근의 데이터 몇 개의 평균을 구하는 것이 미래를 예측할 때는 더 잘 맞을 가능성이 높다.
이전 데이터와 최근의 데이터를 같이 사용할 거라면 최근의 데이터에 가중치를 부여하는 것이 미래를 예측할 때는 더 잘 맞을 가능성이 높다.

1.rolling 함수
=>단순 이동 평균을 계산해주는 함수
=>window 매개변수에 데이터 개수를 설정하면 데이터 개수 단위로 연산을 수행

2.ewm 함수
=>지수 이동 평균을 계산해주는 함수
=>지수 이동 평균은 최근의 데이터에 가중치를 부여하는 방식의 평균
주식 데이터는 이 평균을 사용합니다.
=>기간(span)을 설정하면 아래 수식으로 평균
데이터가 3개인 경우
x1, x2, x3(가장 최근 데이터)
1-span을 알파라고 합니다. 알파를 계산할 때는 1/span 이 span 자리에 대입

x3 + (1-(1-span))x2 + (1-(1-span))제곱x3 / 1 + (1-알파) + (1-알파)제곱


 


















