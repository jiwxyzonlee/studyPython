** 일반적인 python이나 anaconda를 이용해서 설치한 파이썬
- C로 만들어진 python
- C 언어는 소스코드를 가지고 실행 파일을 만들어서 배포
->> 실행 파일을 만들 때 운영체제의 start up 코드가 포함되어야 함
->> 실행 프로그램은 운영체제마다 다름
- windows에서 실행되는 C 언어는 MS-C이고 이 언어로 프로그램을 만드는 대표적인 IDE가 Visual C++ 이고 이 Visual C++로 만들어진 프로그램을 실행시키기 위해서는 재배포 패키지나 build tool이 설치되어 있어야 함


/*
Java -> Source -(Compile)-> Kotlin, Scala // Class + jvm(startup 코드)

C -> Source -(Compile) -> Obj -(build)-> 실행프로그램 + 운영체제의 startup 코드

*/



** 불균형한 클래스의 데이터를 가지고 분석을 해야 하는 경우
- 샘플 자체의 개수가 작을 때 자료를 더 수집하는 것이 가장 좋음
- 데이터를 수집하는 것이 가능하지 않을 때는 데이터에 가중치를 적용해서 사용하거나 분류 알고리즘의 매개변수 중에서 weight가 있으면 이 매개변수가 데이터에 가중치를 적용할 수 있는 매개변수
- 개수가 작은 데이터의 개수를 강제로 늘리거나 개수가 많은 데이터의 개수를 줄이는 업샘플링이나 다운 샘플링을 해서 알고리즘에 적용
- 평가지표를 다양하게 선택
->> 정확도 대신 재현율이나 F1 통계량 등을 이용 <<<<<< 꼭 기억


** 다변량 분석에서 데이터의 상대적 크기 문제

- 다변량 분석
: 2개 이상의 컬럼 데이터를 가지고 분석

- 하나의 컬럼 데이터는 값의 범위가 0-100이고 다른 컬럼의 데이터는 값의 범위가 0-1이라면 이 경우 2개의 데이터를 가지고 다변량 분석을 하게 되면 첫번째 컬럼의 영향을 받게 될 수 있음
->> 이 경우 값의 범위를 일치시켜주는 것이 좋음
- 값의 범위는 같으나 분포가 다른 경우에도 분포를 기준으로 하여 값 조정이 필요함
->> 최댓값으로 나누거나 최댓값-최솟값을 분모로 하고 해당값-최솟값을 분자로 하여 값 조정
- 이러한 값 조정을 scailing이라고 함
->> 0~1 사이나 -1~1사이로 조정
->> 더 큰 값으로도 가능하나 일반적으로 머신러닝에서 값의 크기가 커지면 정확도가 떨어지는 문제


1. 표준화
: 모든 값들의 표준값을 정해서 그 값을 기준으로 차이를 구하여 비교하는 방법

1) 표준값: (데이터-평균)/표준편차
->> 표준 값의 평균은 50

2) 편차값: 표준값 * 10 + 50
->> 위의 숫자보다 큰 숫자로 변환


2. sklearn의 정규화

1) StandardScaler
: 평균이 0이고 표준편차가 1이 되도록 변환
(벡터 - 평균) / 표준편차
->> 주로 주성분 분석에서 많이 이용

2) MinMaxScaler
: 최댓값이 1, 최솟값이 0이 되도록 변환
(벡터 - 최솟값) / (최댓값 - 최솟값)
->> 신경망에서 주로 이용

3) RobustScaler
: 중앙값이 0, IQR(4분위수)
(벡터 - 중간값) / (75% - 25%)
- 앞의 방식들은 이상치에 영향을 많이 받음
- 데이터 분포가 불균형이거나 극단치가 존재하는 경우에 주로 이용

4) Quantile Transformer
: 데이터를 1000개의 분위로 나눈 후 0-1 사이에 고르게 분포시키는 방식
- outlier의 영향을 적게 받기 위해서 사용


** 정규화
- 값의 범위를 0-1 사이의 데이터로 변환
- 표준화는 일정한 범위 내로 데이터를 변환하는 것, 정규화는 0-1 사이로 해야 함
- Normalizer 클래스를 이용해서 transform 메소드에 데이터를 대입하면 됨
->> 이 때 norm 매개변수에 옵션 설정 가능
->> max는 최댓값으로 나누는 방식
->> max를 이용할 때는 하나의 부호 형태의 데이터이어야 함
->> l1과 l2는 거리 계산 방식
->> l1는 맨해튼 거리를 이용하고 l2는 유클리드 거리 이용 (맨해튼 거리는 이동 꼭짓점 3개-선 2개- 계단 거리 총합, 유클리드 거리는 꼭짓점 2개- 선 1개)


** 다항과 교차항
- 기존 데이터에 데이터들을 곱하고 제곱을 하여 데이터를 추가하는 것
- 특성과 타깃 사이에 비선형 관계가 존재할 때 사용하는 방법
- 비선형 관계는 2개의 관계가 직선의 형태가 아니고 곡선의 형태

- 각 특성이 다른 특성에 영향을 줄 때 각 특성을 곱합 교차항을 가지고 인코딩
- 다변량 분석(2개 이상의 컬럼을 가지고 분석)할 때 2개의 컬럼 사이에 상관관계가 있는 경우가 있음
- 이런 경우 2개의 컬럼 모두를 가지고 분석 시 다중공선성 문제가 붉어질 수 있음
->> 어떤 컬럼의 값을 알면 다른 컬럼의 값을 예측할 수 있는 경우 발생할 수 있는 문제
->> 이런 경우 2개의 컬럼을 1개의 컬럼으로 변환하는 작업을 해야 하는데 (차원 축소) 이런 경우 더하거나 곱하거나 제곱하여 새로운 값을 만듦

- PolynomialFeatures 클래스를 이용하는 데 몇 차항까지 생성할 것인지 degree에 설정
->> 첫번째 데이터로 1을 추가할 지 여부를 include_bias에 설정
- 연산식의 순서는 get_feature_names 메소드를 이용해서 확인 가능



** 표준화나 정규화는 직접 하는 경우가 많지만 다항식을 만드는 것은 대부분 머신러닝과 알고리즘에서 자체적으로 처리하는 경우가 많음



** 특성 변환
- 데이터에 동일한 함수를 적용해서 다른 데이터로 직접 변환하는 것
- pandas에서는 apply 메소드를 이용하고 sklearn에서는 preprocessing.FunctionTransform나 ColumnTransformer 클래스를 이용
- FunctionTransformer는 모든 열에 동일한 함수를 적용하고 ColumTransformer는 서로 다른 함수를 적용할 수 있음
->> 객체를 생성할 때 적용할 함수를 설정해서 만들고 transform 메소드에 데이터를 대입하면 됨











