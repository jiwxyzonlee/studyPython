library(wordcloud2)
data("Groceries")
summary(Groceries)
library(dplyr)
glimpse(Groceries)
str(Groceries)
library(dplyr)
mpg <- as.data.frame(ggplot2::mpg)
mpg1 <- mpg
rename(mpg1, city = cty, highway = hwy)
mpg1 <- rename(mpg1, city = cty, highway = hwy)
head(mpg); head(mpg1)
groceries_df <- as(Groceries, "data.frame")
#품목 검사
itemName <- itemLabels(Groceries)
itemCount <- itemFrequency(Groceries)*9835
class(Groceries)
test <- as.data.fame(Groceries)
test <- as.data.frame(Groceries)
itemName
itemCount
wordcloud(words = itemName, freq = itemCount, min.freq = 1,
scale = c(3, 0.2), col = col, random.order = F)
#품목 검사
itemName <- itemLabels(Groceries) #아이템 이름 추출
itemCount <- itemFrequency(Groceries)*9835
#워드클라우드
col <- brewer.pal(8, "Dark2") #다크투라는 팔레트에서
wordcloud(words = itemName, freq = itemCount, min.freq = 1,
scale = c(3, 0.2), col = col, random.order = F)
rules <- apriori(Groceries, parameter = list(support = 0.01,
confidence =0.35))
inspect(rules)
#안을 봅시다
summary(rules)
glimpse(rules)
str(rules)
rpart.plot(rules)
plot(rules)
rules %>% sort(by = "lift") %>% inspect()
inspect(sort(rules, by = "lift"))
rules %>% sort(by = "lift") %>% inspect()
rules %>% sort(by = "lift") %>% inspect() %>% str()
rules %>% sort(by = "lift") %>% inspect() %>% summary()
rules %>% sort(by = "lift") %>% inspect() %>% head()
plot(rules, method = "scatterplot")
plotly_arules(rules, method = "scatterplot",
measure = c("support", "confidence"),
shape = "lift"))
plotly_arules(rules, method = "scatterplot",
measure = c("support", "confidence"),
shape = "lift")
plotly_arules(rules, method = "scatterplot",
measure = c("support", "confidence"),
shading = "lift")
plotly_arules(rules, method = "scatterplot",
measure = c("support", "confidence"),
shading = "lift")
plotly_arules(rules, method = "matrix",
measure = c("support", "confidence"),
shading = "lift")
plot(rules, method = "scatterplot")
plotly_arules(rules, method = "scatterplot",
measure = c("support", "confidence"),
shading = "lift")
plotly_arules(rules, method = "matrix",
measure = c("support", "confidence"),
shading = "lift")
library(readr)
library(readr)
text = read.csv("news2.csv")
text = read.csv("C:\crawl\news2.csv")
text = read.csv("C:\crawl\news2.csv")
text = read.csv("C:/crawlnews2.csv")
text = read.csv("C:/crawl/news2.csv")
View(text)
readr
library(readr)
library(KoNLP)
library(dplyr)
library(memoise)
library(wordcloud2)
text = read.csv("C:/crawl/news2.csv")
useSejongDic()
nouns <- extractNoun(text)
nouns
View(text)
text = read.csv("C:/crawl/news2.csv", header = T)
View(text)
text = read.csv("C:/crawl/news2.csv", stringsAsFactors = F)
View(text)
text = read.csv("C:/crawl/news2.csv", stringsAsFactors = F)
View(text)
text = read.csv("C:/crawl/news2.csv", stringsAsFactors = T)
View(text)
str(text)
View(text)
text %>% select( ,3)
text = text(,3)
text %>% select(c(,3))
text = read.csv("C:/crawl/news2.csv", header = F)
View(text)
text %>% select(V3)
text = text %>% select(V3)
View(text)
nouns <- extractNoun(text)
nouns
text = read.csv("C:/crawl/news2.csv", header = F)
text = text %>% select(V3)
nouns <- extractNoun(text)
nouns <- unlist(nouns)
nouns
nouns <- nouns[(nchar(nouns)>=2)]
nouns <- unlist(nouns)
nouns <- nouns[(nchar(nouns)>=2)]
wordFreq <- table(nouns)
wordFreq <- table(nouns) %>% sort(decreasing = T)
wordFreq %>% head()
text = text %>% select(V3)
nouns <- extractNoun(text)
nouns <- text %>%
extractNoun() %>%
unlist()
nouns[nchar(nouns)>=2] %>%
table() %>%
sort(decreasing=T) %>% #head(20) %>%
wordcloud2(fontFamily = '맑은 고딕')
View(text)
nouns <- text %>%
extractNoun()
text = text %>% select(V3)
nouns <- sapply(text, extractNoun, USE.NAMES = F)
nouns[nchar(nouns)>=2] %>%
table() %>%
sort(decreasing=T) %>% #head(20) %>%
wordcloud2(fontFamily = '맑은 고딕')
nouns[nchar(nouns)>=2] %>%
table()
nouns[nchar(nouns)>=2] %>%
table() %>%
sort(decreasing=T) %>%
wordcloud2(fontFamily = '맑은 고딕')
View(text)
text = text %>% select(V3)
nouns <- sapply(text, extractNoun, USE.NAMES = F)
nouns <- dput(text)
nouns[nchar(nouns)>=2] %>%
table() %>%
sort(decreasing=T) %>%
wordcloud2(fontFamily = '맑은 고딕')
text = read.csv("C:/crawl/news2.csv", header = F, encoding = 'UTF-8')
text = text %>% select(V3)
View(text)
text = read.csv("C:/crawl/news2.csv", header = F)
text = text %>% select(V3)
View(text)
nouns <- sapply(text, extractNoun, USE.NAME = F)
nouns <- sapply(text, extractNoun, USE.NAMEs = F)
View(nouns)
nouns <-lsapply(text, extractNoun)
nouns <-lapply(text, extractNoun)
nouns <- unlist(nouns)
View(nouns)
nouns <-lapply(text, extractNoun)
nouns <- unlist(nouns)
View(nouns)
nouns <- unlist(nouns)
x <- gsub("[^A-Za-z가-힣[:space:][:digit:][:punct:]]", "", nouns)
x <- gsub("@|\n|RT", "", x)
x <- gsub("[[:punct:]]", " ", x)
x <- gsub("[[:digit:]]", "", x)
x <- tolower(x)
x <- gsub("[a-z]", "", x)
x <- str_trim(x)
nouns <-lapply(text, extractNoun, USE.NAMES =F)
library(readr)
library(KoNLP)
library(dplyr)
library(memoise)
library(wordcloud2)
useSejongDic()
text = read.csv("C:/crawl/news2.csv", header = F)
text = text %>% select(V3)
nouns <-lapply(text, extractNoun, USE.NAMES =F)
nouns <-lapply(text, extractNoun)
nouns1 <-lapply(text, extractNoun)
nouns <- unlist(nouns1)
View(nouns1)
nouns1 <-lapply(text, extractNoun)
nouns <- unlist(nouns1)
View(nouns)
nouns1 <-lapply(text, extractNoun, USE.NAMES = F)
nouns1 <-sapply(text, extractNoun, USE.NAMES = F)
nouns <- unlist(nouns1)
View(nouns)
nouns <- gsub("","-",nouns)
nouns
nouns <- str_split(data1. "-")
text = read.csv("C:/crawl/news2.csv", header = F)
library(readr)
library(KoNLP)
library(dplyr)
library(memoise)
library(wordcloud2)
useSejongDic()
text = read.csv("C:/crawl/news2.csv", header = F)
text = text %>% select(V3)
nouns1 <-sapply(text, extractNoun, USE.NAMES = F)
nouns <- gsub("","-",nouns)
nouns <- str_split(data1, "-")
library(stringr)
nouns <- str_split(data1, "-")
nouns <- str_split(nouns, "-")
nouns <- str_replace_all(unlist(nouns), "[:alpha:][:blank:]]","")
text = text %>% select(V3)
nouns <- gsub("","-",nouns)
nouns <- str_split(nouns, "-")
text = read.csv("C:/crawl/news2.csv", header = F)
text = text %>% select(V3)
nouns <- gsub("","-",nouns)
nouns <- str_split(nouns, "-")
nouns <- str_replace_all(unlist(nouns), "[:alpha:][:blank:]]","")
nouns2 <- Map(extractNoun, nouns)
text = text %>% select(V3)
nouns <- str_split(text, "\\w", " ")
text = text %>% select(V3)
nouns <- str_replace_all(text, "\\w", " ")
nouns <- unlist(nouns)
nouns
text = text %>% select(V3)
nouns1 <-sapply(text, extractNoun, USE.NAMES = F)
nouns <- str_replace_all(text, "\\w", " ")
nouns <- unlist(nouns)
nouns
text = read.csv("C:/crawl/news2.csv", header = F)
text = text %>% select(V3)
nouns1 <-sapply(text, extractNoun, USE.NAMES = F)
nouns <- str_replace_all(nouns1, "\\w", " ")
nouns <- unlist(nouns)
nouns
nouns1 <-sapply(text, extractNoun, USE.NAMES = F)
text = text %>% select(V3)
nouns1 <-sapply(text, extractNoun, USE.NAMES = F)
nouns <- str_replace_all(nouns1, "\\w", )
### 뉴스 데이터 텍스트 마이닝
## 텍스트 마이닝 준비하기
# 패키지 설치하기
install.packages("rJava")
install.packages("rJava")
# 패키지 로드하기
library(KoNLP)
library(dplyr)
library(rJava)
library(memoise)
# 사전 설정하기
useNIADic()
## 데이터 준비하기
# 데이터 불러오기
#txt <- readLines("c:/crawl/news.txt")
csv <- read.csv("c:/crawl/news2.csv", header = F)
txt <- csv$V3
head(txt)
length(txt)
library(readr)
library(KoNLP)
library(dplyr)
library(memoise)
library(wordcloud2)
library(stringr)
## 데이터 준비하기
# 데이터 불러오기
#txt <- readLines("c:/crawl/news.txt")
csv <- f.read("c:/crawl/news2.csv", header = F)
## 데이터 준비하기
# 데이터 불러오기
#txt <- readLines("c:/crawl/news.txt")
csv <- fread("c:/crawl/news2.csv", header = F)
library(read.table)
## 데이터 준비하기
# 데이터 불러오기
#txt <- readLines("c:/crawl/news.txt")
csv <- fread("c:/crawl/news.csv", header = F)
## 데이터 준비하기
# 데이터 불러오기
#txt <- readLines("c:/crawl/news.txt")
csv <- fread("c:/crawl/news.csv")
library(data.table)
## 데이터 준비하기
# 데이터 불러오기
#txt <- readLines("c:/crawl/news.txt")
csv <- fread("c:/crawl/news.csv")
## 데이터 준비하기
# 데이터 불러오기
#txt <- readLines("c:/crawl/news.txt")
csv <- fread("c:/crawl/news2.csv")
## 데이터 준비하기
# 데이터 불러오기
#txt <- readLines("c:/crawl/news.txt")
setwd("C:/crawl")
csv <- fread("c:/crawl/news2.csv")
csv <- fread("news2.csv")
getwd()
csv <- fread("news2.csv")
csv <- fread("news2.csv", sep = ',')
csv <- readr("news2.csv", header = F)
csv <- read.csv("news2.csv", header = F)
txt <- csv$V3
head(txt)
length(txt)
txt <- str_replace_all(txt, "\\W", " ")
head(txt)
## 가장 많이 사용된 단어 알아보기
# 명사 추출하기
nouns <- extractNoun(txt)
head(nouns)
# 추출한 명사 list를 문자열 벡터로 변환, 단어별 빈도표 생성하기
wordcount <- table(unlist(nouns))
head(wordcount)
tail(wordcount, 20)
# 데이터 프레임으로 변환하기
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
head(df_word)
tail(df_word)
str(df_word)
# 변수명 수정하기
df_word <- rename(df_word,
word=Var1,
freq= Freq)
tail(df_word)
# 두 글자 이상 단어 추출하기
df_word <- filter(df_word, nchar(word) >= 2)
# 빈도 순으로 정렬한 후 상위 20개 단어 추출하기
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
head(top_20)
csv <- read.csv("news2.csv", header = F)
txt <- csv %>% select(V3)
head(txt)
length(txt)
txt <- str_replace_all(txt, "\\W", " ")
## 가장 많이 사용된 단어 알아보기
# 명사 추출하기
nouns <- extractNoun(txt)
head(nouns)
# 추출한 명사 list를 문자열 벡터로 변환, 단어별 빈도표 생성하기
wordcount <- table(unlist(nouns))
head(wordcount)
tail(wordcount, 20)
txt <- csv$V3
head(txt)
length(txt)
txt <- str_replace_all(txt, "\\W", " ")
head(txt)
## 가장 많이 사용된 단어 알아보기
# 명사 추출하기
nouns <- extractNoun(txt)
head(nouns)
# 추출한 명사 list를 문자열 벡터로 변환, 단어별 빈도표 생성하기
wordcount <- table(unlist(nouns))
head(wordcount)
tail(wordcount, 20)
# 데이터 프레임으로 변환하기
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
head(df_word)
tail(df_word)
str(df_word)
# 변수명 수정하기
df_word <- rename(df_word,
word=Var1,
freq= Freq)
tail(df_word)
# 두 글자 이상 단어 추출하기
df_word <- filter(df_word, nchar(word) >= 2)
# 빈도 순으로 정렬한 후 상위 20개 단어 추출하기
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
head(top_20)
# 막대 그래프 그리기
library(ggplot2)
library(ggthemes)
ggplot(data = top_20, aes(x=word, y=freq, fill=word)) +
geom_bar(stat = "identity") +
geom_text(aes(label=freq), vjust=-0.3)  #빈도 표시
order <- arrange(top_20, freq)$word       #빈도 순서 변수 생성하기
ggplot(data = top_20, aes(x=word, y=freq, fill=word)) +
geom_col() +                            #막대그래프(x축,y축을 모두 설정)
scale_x_discrete(limit=order) +         #빈도순으로 막대 정렬
#ylim(0, 50) +                           #y축의 범위 지정
coord_flip() +                          #x축과 y축의 구성을 뒤집어 표현
geom_text(aes(label=freq), hjust=-0.3)  #빈도 표시
## 워드 클라우드 만들기
# 패키지 설치/로드 하기
#install.packages("wordcloud")
library(wordcloud)
# 단어 색상 목록 만들기
pal <- brewer.pal(8, "Dark2")
# 난수 고정하기
set.seed(1234)
View(txt)
csv <- read.csv("news2.csv", header = F)
txt <- csv$V3
txt <- str_replace_all(txt, "\\W", " ")
txt <- gsub("\\d+", txt)
txt <- str_replace_all(txt, "\\W", " ")
txt <- gsub("\\d+",'', txt)
head(txt)
txt <- str_replace_all(txt, "\\W", " ") #특수문자 제외
csv <- read.csv("news2.csv", header = F)
txt <- csv$V3
txt <- str_replace_all(txt, "\\W", " ") #특수문자 제외
txt <- gsub("\\d+",'', txt) #숫자제외
txt <- gsub("[A-Za-z]",'',txt)
head(txt)
## 가장 많이 사용된 단어 알아보기
# 명사 추출하기
nouns <- extractNoun(txt)
head(nouns)
# 추출한 명사 list를 문자열 벡터로 변환, 단어별 빈도표 생성하기
wordcount <- table(unlist(nouns))
head(wordcount)
tail(wordcount, 20)
# 데이터 프레임으로 변환하기
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
head(df_word)
tail(df_word)
str(df_word)
# 변수명 수정하기
df_word <- rename(df_word,
word=Var1,
freq= Freq)
tail(df_word)
# 두 글자 이상 단어 추출하기
df_word <- filter(df_word, nchar(word) >= 2)
# 빈도 순으로 정렬한 후 상위 20개 단어 추출하기
top_20 <- df_word %>% arrange(desc(freq)) %>% head(20)
head(top_20)
ggplot(data = top_20, aes(x=word, y=freq, fill=word)) +
geom_bar(stat = "identity") +
geom_text(aes(label=freq), vjust=-0.3)  #빈도 표시
ggplot(data = top_20, aes(x=word, y=freq, fill=word)) +
geom_col() +                            #막대그래프(x축,y축을 모두 설정)
scale_x_discrete(limit=order) +         #빈도순으로 막대 정렬
#ylim(0, 50) +                           #y축의 범위 지정
coord_flip() +                          #x축과 y축의 구성을 뒤집어 표현
geom_text(aes(label=freq), hjust=-0.3)  #빈도 표시
## 워드 클라우드 만들기
# 패키지 설치/로드 하기
#install.packages("wordcloud")
library(wordcloud)
library(RColorBrewer)
# 단어 색상 목록 만들기
pal <- brewer.pal(8, "Dark2")
# 난수 고정하기
set.seed(1234)
# 워드 클라우드 만들기
wordcloud(words = df_word$word,    #단어
freq = df_word$freq,     #빈도
min.freq = 2,            #최소 단어 빈도
max.words = 200,         #표현 단어 수
random.order = F,        #고빈도 단어 중앙 배치
rot.per = .1,            #회전 단어 비율
scale = c(4, 0.3),       #단어 크기 범위
colors = pal)            #색상 목록
# 단어 색상 바꾸기
pal <- brewer.pal(9, "Blues")[5:9] #색상 목록 생성
set.seed(1234)                     #난수 고정
wordcloud(words = df_word$word,    #단어
freq = df_word$freq,     #빈도
min.freq = 2,            #최소 단어 빈도
max.words = 200,         #표현 단어 수
random.order = F,        #고빈도 단어 중앙 배치
rot.per = .1,            #회전 단어 비율
scale = c(4, 0.3),       #단어 크기 범위
colors = pal)            #색상 목록
# 워드 클라우드 만들기
wordcloud(words = df_word$word,    #단어
freq = df_word$freq,     #빈도
min.freq = 2,            #최소 단어 빈도
max.words = 500,         #표현 단어 수
random.order = F,        #고빈도 단어 중앙 배치
rot.per = .1,            #회전 단어 비율
scale = c(4, 0.3),       #단어 크기 범위
colors = pal)            #색상 목록
# 빈도 순으로 정렬한 후 상위 20개 단어 추출하기
top_20 <- df_word %>% arrange(desc(freq)) #%>% head(20)
head(top_20)
ggplot(data = top_20, aes(x=word, y=freq, fill=word)) +
geom_bar(stat = "identity") +
geom_text(aes(label=freq), vjust=-0.3)  #빈도 표시
# 빈도 순으로 정렬한 후 상위 20개 단어 추출하기
top_100 <- df_word %>% arrange(desc(freq)) %>% head(100)
ggplot(data = top_100, aes(x=word, y=freq, fill=word)) +
geom_bar(stat = "identity") +
geom_text(aes(label=freq), vjust=-0.3)  #빈도 표시
order <- arrange(top_100, freq)$word       #빈도 순서 변수 생성하기
ggplot(data = top_100, aes(x=word, y=freq, fill=word)) +
geom_col() +                            #막대그래프(x축,y축을 모두 설정)
scale_x_discrete(limit=order) +         #빈도순으로 막대 정렬
#ylim(0, 50) +                           #y축의 범위 지정
coord_flip() +                          #x축과 y축의 구성을 뒤집어 표현
geom_text(aes(label=freq), hjust=-0.3)  #빈도 표시
# 단어 색상 목록 만들기
pal <- brewer.pal(8, "Dark2")
# 난수 고정하기
set.seed(1234)
# 워드 클라우드 만들기
wordcloud(words = df_word$word,    #단어
freq = df_word$freq,     #빈도
min.freq = 2,            #최소 단어 빈도
max.words = 500,         #표현 단어 수
random.order = F,        #고빈도 단어 중앙 배치
rot.per = .1,            #회전 단어 비율
scale = c(4, 0.3),       #단어 크기 범위
colors = pal)            #색상 목록
