*pdf 3.pandas의 자료구조

** 배열의 결합


1. hstack
: 행의 수가 같은 배열을 열 방향으로 결합해주는 함수


2. vstack
: 열의 수가 같은 배열을 행 방향으로 결합해주는 함수


3. dstack
: 행과 열의 수가 같은 배열을 가지고 위치가 같은 요소들을 가지고 하나의 배열로 묶어서 차원이 1개 늘어난 배열을 만들어주는 함수


4. stack
: dstack과 비슷하지만 원하는 방향으로 새로운 배열을 만들어서 차원을 늘려주는 함수


import numpy as np

# 일련 번호 형태로 배열 만들기
ar = np.arange(1, 13)

# 3행 4열로 변경

ar = ar.reshape(3, 4)
print(ar)
print()

# 일정한 간격을 가지고 만들기
br = np.linspace(101, 112, num = 12)
br = br.reshape(3, 4)
print(br)
print()

# 행 방향으로 합치기
print(np.hstack([ar, br]))
print()

# 열 방향으로 합치기
print(np.vstack([ar, br]))



**pandas의 자료구조
- Series와 Dataframe

- numpy의 ndarray는 행이나 열을 구분하는 것이 정수로 된 index
- pandas의 자료구조들은 index를 직접 설정 가능


1. Series

1) 생성

Series(data, index=None, dtype=None, copy=False)
->> data는 __iter__가 구현된 객체
->> index는 데이터별 이름을 부여하는 것, 생략 시 0부터 시작하는 숫자
->> dtype은 각 요소들의 자료형, 생략 시 유추하여 설정
->> copy는 복제 여부

- data에 dict를 대입 시 key가 index가 되고 value가 데이터로 설정됨
- values 속성을 호출 시 데이터만 추출하여 numpy의 ndarray로 리턴
- index 속성을 호출 시 index 들을 리턴하여 직접 설정 가능
- 각각의 데이터는 [인덱스] 를 이용하여 접근
- 범위에 해당하는 데이터에 접근할 때는 [정수 인덱스:정수 인덱스] 의 경우, 정수 인덱스 앞까지이지만 인덱스를 문자열로 생성하여 대입할 시 [문자열 인덱스:문자열 인덱스] 는 마지막 문자열 인덱스도 포함함
[0:4] -> 0, 1, 2, 3
['apple':'mango'] -> apple, ... , mango
- 인덱스 자리에 list 를 대입해서 list 에 포함된 인덱스들의 데이터만 추출 가능

# Series와 DataFrame은 현재 모듈에 포함
from pandas import Series, DataFrame
# Series와 DataFrame을 제외하고는 pd로 접근
import pandas as pd

# Series 만들기
good1 = Series([1000, 1500, 500])
print(good1) ; print()

# 1개의 데이터 접근
print(good1[0]) ; print()

# 연속된 범위의 데이터 접근
print(good1[0:2]) ; print()

# 불연속된 데이터 접근
print(good1[[0, 2]]) ; print()

# 인덱스 설정
good1.index = ['Orange', 'Apple', 'Banana']
print(good1) ; print()
print(good1['Orange':'Banana']) ; print()
# Orange부터 Banana까지

2) 연산

- Series와 Scala Data의 연산은 Broadcast 연산(각각의 요소와 연산해서 결과를 Series로 리턴)
- Series 끼리의 연산은 index 기준으로 연산
->> None(numpy.nan) 과의 연산 결과는 무조건 None
- 없는 index 와의 연산 결과도 None


2. DataFrame
- 인덱스와 컬럼이름을 가진 행렬(2차원 배열)
->> 테이블
- dict의 list와 유사
- 보통 외부 데이터를 가지고 생성
- 직접 생성하는 경우 dict 이용
->> 하나의 key에 list 형태의 value를 설정해서 생성
- 통계에서 주로 사용


# Dictionary 생성
items = {'name': ['orange', 'mango'],
        'price':[2000, 2500],
        'manufacture':['korea', 'taiwan']}
# DataFrame 생성
df = DataFrame(items)

print(df)

1) 생성 가능한 데이터
- dict
- 1차원 ndarray
- Series의 list
- dict, list, tuple, set의 list

2) index
- 행에 붙이는 이름
- 직접 설정하지 않으면 0부터 시작하는 정수로 된 인덱스 생성
- 생성 시 index 매개변수를 이용하여 직접 설정 가능
- 생성 후 index 속성을 이용하여 설정 가능

3) 컬럼이름
- 열에 붙이는 이름
- 직접 설정하지 않으면 디폴트로 생성
: dict의 경우 key가 컬럼이름으로 설정, 그 이외는 정수로 설정
- 생성 시 columns 매개변수를 이용하여 설정 가능
- 생성 후 columns 속성을 이용하여 설정 가능

4) 테이블의 데이터 일부 확인
head(데이터 개수)
: 앞에서부터 데이터 개수만큼 리턴

tail(데이터 개수)
: 뒤에서부터 데이터 개수만큼 리턴

5) info()
- DataFrame의 개요 리턴
- 외부에서 데이터를 가져와서 DataFrame을 만들 때 head나 tail을 이용해서 데이터가 제대로 불려 왔는지 확인하고 데이터의 개요를 확인하기 위해서 info() 를 호출하여 출력

# Dictionary 생성
items = {'name': ['orange', 'mango', 'apple', 'banana'],
        'price':[2000, 2500, 3000, 500],
        'manufacture':['korea', 'taiwan', 'korea', 'vietnam']}
# DataFrame 생성
df = DataFrame(items)

#print(df)

# 인덱스 직접 설정
df.index = np.arange(10, 50, 10)
print(df); print()
print(df.head(3)) ; print()
print(df.info()); print()

"""
<class 'pandas.core.frame.DataFrame'>
Int64Index: 4 entries, 10 to 40  - 총 4개의 데이터, 10부터 40까지 인덱스
Data columns (total 3 columns):
name           4 non-null object
price          4 non-null int64
manufacture    4 non-null object
dtypes: int64(1), object(2)  - 숫자 이외의 자료형은 무조건 object
memory usage: 128.0+ bytes
None

"""

- 개요 확인

<class 'pandas.core.frame.DataFrame'>
->> 전체 데이터의 자료형

Int64Index: 4 entries, 10 to 40 
->> 행의 개수와 인덱스에 대한 내용

Data columns (total 3 columns):
->> 컬럼의 개수

name           4 non-null object
->> name이라는 컬럼은 4개의 데이터가 존재하고 자료형은 객체(문자열)

price          4 non-null int64
->> price라는 컬럼은 4개의 데이터가 존재하고 자료형은 정수

manufacture    4 non-null object
->> manufacture라는 컬럼은 4개의 데이터가 존재하고 자료형은 객체(문자열)

dtypes: int64(1), object(2)
->> 1개의 컬럼은 정수로 되어있고 2개의 컬럼은 객체

memory usage: 128.0+ bytes
->> 메모리 사용량
None
->> 각 컬럼의 None의 개수를 파악할 수 있음

6)

# sklearn에서 제공하는 boston 주택 가격 데이터 가져오기
from pandas import Series, DataFrame
import pandas as pd
import numpy as np

# sklearn에서 데이터를 쓰기 위한 import
from sklearn import datasets

boston = datasets.load_boston()
# datasets.load_데이터 이름()

# 자료형 확인
print(type(boston)) ; print() # <class 'sklearn.utils.Bunch'>

# 데이터 확인
print(type(boston.keys)) ; print() # <class 'builtin_function_or_method'>
print(boston.keys) ; print() # <built-in method keys of Bunch object at 0x000001E38C39F168>
print(boston.keys()) ; print() #dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])

# 실제 데이터의 자료형 확인
print(type(boston.data)) ; print()  #<class 'numpy.ndarray'>
# DataFrame으로 생성가능한지 확인 (shape 확인)
print(boston.data.shape)  ; print()  #(506, 13)

# 가져온 데이터셋이 ndarray인 경우 데이터를 파악하기가 어려울 수 있어서
# 컬럼이름을 부여한 DataFrame으로 가지고 있는 것이 데이터 파악에 용이
# 머신 러닝에 적용할 때는 df.values 로 다시 ndarray로 만들면 됨
df = DataFrame(boston.data
               , columns = ['CRIM' ,'INDUS', 'NOX', 'RM'
                            , 'LSTAT', 'B', 'PTRATIO', 'ZN'
                            , 'CHAS', 'AGE', 'RAD', 'DIS', 'TAX'])
print(df.info()) ; print()

7) fwf 파일
: 텍스트 파일, 글자 수가 일정한 형태로 되어 있는 파일

pandas.read_fwf(파일 경로, widths=(글자수를 나열), names=(컬럼이름 나열), encoding=인코딩방식)

- 파일이 windows에서 기본으로 인코딩으로 만들어졌다면 cp949(ms949)이고 그 이외의 운영체제에서 만들어졌다면 utf-8

# data/data_fwf.txt 파일의 내용을 가지고 DataFrame 만들기
# 텍스트를 한 번에 읽은 후 슬라이싱을 이용해서 만들기도 함

fwf = pd.read_fwf('./data/data_fwf.txt'
                  , width = (10, 2.5)
                  , names = ('날짜', '종목이름', '종가')
                  , encoding = 'utf-8')
# 한글 깨지면 encoding 필요

print(fwf) ; print()

8) csv 파일
- read_csv: 기본 구분자가 ','로 설정
- read_table: 기본 구분자가 Tab
- 옵션 설정이 없으면 첫번째 행의 데이터가 컬럼이 됨

# item.csv 파일을 읽어서 DataFrame 만들기
# 파일 구분자는 메모장으로 열어보면 잘 보임
item = pd.read_csv('./data/item.csv')
print(item.head()) ; print()
print(item.info()) ; print()

- read_csv의 매개변수
- path: 파일의 경로
- sep: 구분자(기본은 ',')
- header: 컬럼이름의 행 번호로 기본값은 0, 첫번째 행이 컬럼이름이 아닌 경우는 None으로 설정
- index_col: 인덱스로 사용할 컬럼 번호나 컬럼 이름
- names: 컬럼이름으로 사용할 list, header=None과 함께 사용
- skiprows: 읽지 않을 행의 개수
- nrows: 일부분만 읽는 경우 읽을 행의 개수
- skip_footer: 마지막에 읽지 않을 행의 개수
- encoding: 인코딩 설정
- squeze: 행이 하나인 경우 Series를 만들 때 이 속성을 True로 설정하면 DataFrame으로 생성
- thousands: 천 단위 구분 기호 설정, 이 설정을 하지 않은 상태에서 ','가 있는 데이터를 읽으면 문자열로 읽게 됨
- na_valuse: None 처리할 데이터의 list
- parse_dates: 날짜 형식의 데이터를 datetime으로 변환할지 여부


# item.csv 파일을 읽어서 DataFrame 만들기
# 파일 구분자는 메모장으로 열어보면 잘 보임
item = pd.read_csv('./data/item.csv')
print(item.head()) ; print()
#print(item.info()) ; print()

good = pd.read_csv('./data/item.csv'
                   , header = None
                   , names = ['제품명', '수량', '가격'])
print(good.head()) ; print()
print(good.info()) ; print()


- 텍스트 파일의 용량이 큰 경우(데이터가 많을 때)
->> log 분석을 하기 위해서 csv를 읽어오는 경우 실무에서 사용하는 데이터는 아주 큰 용량임
->> 한국 ebay의 경우 하루 로그기록하는 파일의 사이즈가 대략 8GB
->> 대용량의 파일을 한 번에 읽어서 DataFrame을 만들게 되면 시스템이 멈추거나 작업이 아주 느려짐
->> 이런 파일은 분할해서 읽어야 함

->> nrows와 skiprows를 이용할 수 있음
->> 읽을 때 chunksize(한 번에 읽을 행의 개수)를 설정
->> 이 옵션 이용 시 TextParser 객체가 리턴
->> TextParser를 iterator(for)를 돌리면 chunksize 단위로 데이터를 가져오게 됨


# good.csv 파일의 내용을 2개씩 읽어오기
goodchunk = pd.read_csv('./data/item.csv'
                   , chunksize=2
                   , header=None)
print(goodchunk) #<pandas.io.parsers.TextFileReader object at 0x0000023CBC8525C8>
print()

for two in goodchunk:
    print(two)


-  문자열이 작은 따옴표나 큰 따옴표로 묶여 있는 경우 또는 구분자가 1글자가 아니고 여러 글자인 경우에는 read_csv가 제대로 데이터를 읽지 못하는 경우가 발생할 수도 있음
- csv 모듈의 reader 객체를 이용해서 줄 단위로 읽어 처리해야 함

# 문자열이 작은 따옴표로 묶여 있는 경우
fruits = pd.read_csv('./data/fruit.csv'
                     , header=None
                     , sep = '|')
print(fruits)
print()

# 줄 단위로 읽어서 DataFrame 만들기
import csv
lines = list(csv.reader(open('./data/fruit.csv')
                        , delimiter='|'))
print(lines)
print()

fruits = DataFrame(lines, columns=['이름', '수량', '가격'])
print(fruits)


9) csv 저장
- Series나 DataFrame 객체가 to_csv 함수를 호출
- 첫번째 매개변수는 파일 경로
- sep는 저장될 때의 구분자를 설정
- na_rep는 None 값을 어떻게 출력할 것인지를 설정
- index = None을 설정하면 인덱스는 출력되지 않음
- header=None을 설정하면 컬럼이름이 출력되지 않음
- cols에 컬럼이름의 list를 설정하면 설정한 컬럼이름들만 출력

# 파일에 저장
fruits.to_csv('fruits.csv')

10) excel 파일 읽기
- anaconda를 설치하지 않은 경우 xlrd 패키지를 설치해야 함
- pandas.read_excel(엑셀 파일 경로)
- pandas.io.excel.read_excel

->> 모듈이 다름
- 함수들의 매개변수는 reae_csv 와 유사한데 sheet_name 옵션에 읽을 sheet 이름을 설정해야 함

11) excel 파일 저장
- ExcelWriter 객체를 만들고 DataFrame.to_excel(ExcelWrite객체, sheet_name='시트이름')


- 실습 (excel.xlsx 파일 읽기)
->> 한글이 포함되어 있고 첫줄이 컬럼의 이름인 것을 확인
->> 인코딩 고려

# 엑셀 파일 읽고 쓰기
excel = pd.read_excel('./data/excel.xlsx'
                      , sheet_name = 'Sheet1')
print(excel)

# 엑셀로 저장하기
writer = pd.ExcelWriter('excel.xlsx')
excel.to_excel(writer, sheet_name='연습')
writer.save()

12) HTML 페이지의 table 태그의 내용을 DataFrame으로 변환
pandas.read_html(경로)
- 천단위 구분기호 설정, 인코딩 방식, na 데이터 설정 등의 옵션 등이 있음
- 하나의 페이지에 table 태그가 여러 개 있을 수도 있기 때문에 DataFrame의 list로 리턴

# pandas의 read_html 함수 도움말
#help(pd.read_html)

li = pd.read_html('https://ko.wikipedia.org/wiki/%EC%9D%B8%EA%B5%AC%EC%88%9C_%EB%82%98%EB%9D%BC_%EB%AA%A9%EB%A1%9D'
                  , thousands=',')
#for df in li:
#    print(df.head())

print(li[0])

13) json 데이터 읽기

- pandas.read_json을 이용해서 읽음
- 대다수의 데이터는 배열의 형태가 아니고 일반 객체 형태라서 바로 만들면 나중에 가공작업이 필요한 경우가 많음


list: 인덱스로 구분
dict: key로 구분

14) XML 읽기
- pandas에는 xml을 읽어서 만드는 함수는 없음
->> xml 파싱을 수행한 후 직접 만들어야 함